{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "80d5fd2e-343c-4078-bcc8-13a8da0abfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "from itertools import combinations_with_replacement\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be270aa-d7d2-44db-9918-3c8d1cd47e55",
   "metadata": {},
   "source": [
    "# Read-in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "71e9a37e-4ee4-436f-a57d-45485ac3d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"../data/adjs_conc-abs.csv\")\n",
    "a2 = pd.read_csv(\"../data/adjs_phys-soc.csv\")\n",
    "n = pd.read_csv(\"../data/nouns_conc-abs.csv\")\n",
    "n2 = pd.read_csv(\"../data/nouns_ani.csv\")\n",
    "s = pd.read_csv(\"../data/syntax.csv\")\n",
    "v = pd.read_csv(\"../data/verbs_conc-abs.csv\")\n",
    "v2 = pd.read_csv(\"../data/verbs_phys-psych.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a04c8c-85b3-4df7-bdb6-0508cc81cedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e96bb073-3992-4045-b633-6b69e8621c77",
   "metadata": {},
   "source": [
    "# Make unordered pairs with self pairs\n",
    "\n",
    "If you have 40 unique words, and you want all unordered pairs without self-pairs, the math is:\n",
    "\n",
    "$$\n",
    "\\binom{41}{2} = \\frac{40 \\times 41}{2} = 820\n",
    "$$\n",
    "\n",
    "\n",
    "Explanation:\n",
    "$$\n",
    "\\binom{n + 1}{2} = \\frac{n(n-1)}{2}\n",
    "$$\n",
    "\n",
    "is the number of combinations of \n",
    "ð‘›\n",
    "n elements taken 2 at a time.\n",
    "\n",
    "This avoids:\n",
    "- self-pairs like (apple, apple),\n",
    "- duplicate unordered pairs like (apple, banana) and (banana, apple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1ee3ef8-f362-467b-98c7-a2fd59033403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023940d-2a61-461b-b0fd-616a804fd66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34c9930d-a8cc-4d0a-9aec-573eeeb72d10",
   "metadata": {},
   "source": [
    "# unordered pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bd800886-9c75-42c8-9b35-358fd56e26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "def add_feature_match_specific(df_list, save_path_prefix):\n",
    "    results = {}  # To store split DataFrames\n",
    "\n",
    "    for df in df_list:\n",
    "        base_name = getattr(df, 'name', 'df')\n",
    "        unique_values = df['FeatureMatch'].unique()\n",
    "\n",
    "        for val in unique_values:\n",
    "            split_df = df[df['FeatureMatch'] == val].copy()\n",
    "\n",
    "            # Save to CSV\n",
    "            save_path = f\"{save_path_prefix}{base_name}_{val}.csv\"\n",
    "            split_df.to_csv(save_path, index=False)\n",
    "            print(f\"Saved: {save_path}\")\n",
    "\n",
    "            # Store for later use\n",
    "            results[f\"{base_name}_{val}\"] = split_df\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_and_save_pairwise_dfs(df_list, save_path_prefix=\"../exp_files/\"):\n",
    "    all_split_results = {}  # To collect all final split DataFrames\n",
    "\n",
    "    for df in df_list:\n",
    "        if not hasattr(df, 'name'):\n",
    "            raise ValueError(\"Each DataFrame in df_list must have a `.name` attribute.\")\n",
    "        print(f\"Processing {df.name}...\")\n",
    "\n",
    "        # Use combinations to avoid ordered pairs (no (A,B) and (B,A), only one of them)\n",
    "        pairs = list(itertools.combinations(df.itertuples(index=False), 2))\n",
    "        print(f\"{df.name}: {len(pairs)} unique unordered pairs\")\n",
    "\n",
    "        df_pairs = pd.DataFrame([\n",
    "            {\n",
    "                'Word1': p1.Word,\n",
    "                'Word2': p2.Word,\n",
    "                'FeatureCombo1': p1.FeatureCombo,\n",
    "                'FeatureCombo2': p2.FeatureCombo\n",
    "            }\n",
    "            for p1, p2 in pairs\n",
    "        ])\n",
    "        df_pairs.name = df.name\n",
    "        print(f\"{df.name}: {len(df_pairs)} rows in pair DataFrame\")\n",
    "\n",
    "        df_pairs['FeatureMatch'] = df_pairs.apply(\n",
    "            lambda row: classify_feature_match(row['FeatureCombo1'], row['FeatureCombo2']),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        df_pairs['FeatureMatch'] = df_pairs.apply(\n",
    "            lambda row: 'SelfPair' if row['Word1'] == row['Word2'] else row['FeatureMatch'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Collect results from each processed df\n",
    "        split_results = add_feature_match_specific([df_pairs], save_path_prefix)\n",
    "        all_split_results.update(split_results)\n",
    "\n",
    "    return all_split_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "038fb10c-08a6-4621-912f-5a1ddd15e90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing v...\n",
      "v: 780 unique unordered pairs\n",
      "v: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/v_MaxMatch.csv\n",
      "Saved: ../exp_files/v_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/v_MaxMismatch.csv\n",
      "Saved: ../exp_files/v_ValenceMatchingOnly.csv\n",
      "Processing v2...\n",
      "v2: 780 unique unordered pairs\n",
      "v2: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/v2_MaxMatch.csv\n",
      "Saved: ../exp_files/v2_ValenceMatchingOnly.csv\n",
      "Saved: ../exp_files/v2_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/v2_MaxMismatch.csv\n",
      "Processing n...\n",
      "n: 780 unique unordered pairs\n",
      "n: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/n_MaxMatch.csv\n",
      "Saved: ../exp_files/n_MaxMismatch.csv\n",
      "Saved: ../exp_files/n_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/n_ValenceMatchingOnly.csv\n",
      "Processing n2...\n",
      "n2: 780 unique unordered pairs\n",
      "n2: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/n2_MaxMatch.csv\n",
      "Saved: ../exp_files/n2_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/n2_ValenceMatchingOnly.csv\n",
      "Saved: ../exp_files/n2_MaxMismatch.csv\n",
      "Processing s...\n",
      "s: 780 unique unordered pairs\n",
      "s: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/s_MaxMatch.csv\n",
      "Saved: ../exp_files/s_ValenceMatchingOnly.csv\n",
      "Saved: ../exp_files/s_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/s_MaxMismatch.csv\n",
      "Processing a...\n",
      "a: 780 unique unordered pairs\n",
      "a: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/a_MaxMatch.csv\n",
      "Saved: ../exp_files/a_MaxMismatch.csv\n",
      "Saved: ../exp_files/a_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/a_ValenceMatchingOnly.csv\n",
      "Processing a2...\n",
      "a2: 780 unique unordered pairs\n",
      "a2: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/a2_MaxMatch.csv\n",
      "Saved: ../exp_files/a2_MaxMismatch.csv\n",
      "Saved: ../exp_files/a2_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/a2_ValenceMatchingOnly.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'v_MatchType1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m results \u001b[38;5;241m=\u001b[39m process_and_save_pairwise_dfs([v, v2, n, n2, s, a, a2])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Now `results` is a dictionary where keys are like 'v_SomeFeatureMatch'\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# and values are the corresponding split DataFrames\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv_MatchType1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'v_MatchType1'"
     ]
    }
   ],
   "source": [
    "# Set .name attributes\n",
    "for df, name in zip([v, v2, n, n2, s, a, a2], ['v', 'v2', 'n', 'n2', 's', 'a', 'a2']):\n",
    "    df.name = name\n",
    "\n",
    "# Process and capture results\n",
    "results = process_and_save_pairwise_dfs([v, v2, n, n2, s, a, a2])\n",
    "\n",
    "# Now `results` is a dictionary where keys are like 'v_SomeFeatureMatch'\n",
    "# and values are the corresponding split DataFrames\n",
    "\n",
    "# Example usage:\n",
    "# results['v_MatchType1'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0504d3df-2e64-457d-b4ec-40bd442b79d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4144a-6174-482a-92de-589d9efd74f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de2e48-3fa2-49a6-b263-99cbeafd98cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ecd2c-096f-4596-952e-1c1b030a105c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f5560f0e-c948-472d-b6f0-73c3c923d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You want to cycle only within the first n letters (A to J), \n",
    "# and then loop back around to A (not K, L, etc.) \n",
    "# but track how many loops have occurred to produce AA, BB, etc.\n",
    "\n",
    "# You want to cycle through Aâ€“J repeatedly for each Word1 group â€” just with a shifting starting letter per group.\n",
    "# The label length should stay 1 letter (Aâ€“J) \n",
    "# until the 10th unique value, \n",
    "# at which point we start doubling (AAâ€“JJ), and later tripling, and so on.\n",
    "\n",
    "def generate_group_labels_strict(n, group_index, base_letters):\n",
    "    \"\"\"\n",
    "    Generate n group labels from a base_letters subset (e.g., A-J), shifting the start\n",
    "    per group_index, and repeating letters (A, AA, AAA...) only after 10 groups.\n",
    "    \"\"\"\n",
    "    base_len = len(base_letters)\n",
    "    labels = []\n",
    "\n",
    "    for i in range(n):\n",
    "        pos = (group_index + i) % base_len\n",
    "        repeat = (group_index) // base_len + 1  # only increase length every 10 groups\n",
    "        label = base_letters[pos] * repeat\n",
    "        labels.append(label)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_feature_match_specific(df_list, save_path_prefix):\n",
    "    base_letters = list(string.ascii_uppercase[:10])  # Aâ€“J\n",
    "\n",
    "    for df in df_list:\n",
    "        base_name = getattr(df, 'name', 'df')\n",
    "        unique_values = df['FeatureMatch'].unique()\n",
    "\n",
    "        for val in unique_values:\n",
    "            split_df = df[df['FeatureMatch'] == val].copy()\n",
    "\n",
    "            group_labels = []\n",
    "            for group_index, word1_val in enumerate(split_df['Word1'].unique()):\n",
    "                group_size = (split_df['Word1'] == word1_val).sum()\n",
    "                labels = generate_group_labels_strict(group_size, group_index, base_letters)\n",
    "                group_labels.extend(labels)\n",
    "\n",
    "            split_df['Group'] = group_labels\n",
    "\n",
    "            save_path = f\"{save_path_prefix}{base_name}_{val}.csv\"\n",
    "            split_df.to_csv(save_path, index=False)\n",
    "            print(f\"Saved: {save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_and_save_pairwise_dfs(df_list, save_path_prefix=\"../exp_files/\"):\n",
    "    for df in df_list:\n",
    "        if not hasattr(df, 'name'):\n",
    "            raise ValueError(\"Each DataFrame in df_list must have a `.name` attribute.\")\n",
    "        print(f\"Processing {df.name}...\")\n",
    "\n",
    "        # Use combinations to avoid ordered pairs (no (A,B) and (B,A), only one of them)\n",
    "        pairs = list(itertools.combinations(df.itertuples(index=False), 2))\n",
    "        print(f\"{df.name}: {len(pairs)} unique unordered pairs\")\n",
    "\n",
    "        df_pairs = pd.DataFrame([\n",
    "            {\n",
    "                'Word1': p1.Word,\n",
    "                'Word2': p2.Word,\n",
    "                'FeatureCombo1': p1.FeatureCombo,\n",
    "                'FeatureCombo2': p2.FeatureCombo\n",
    "            }\n",
    "            for p1, p2 in pairs\n",
    "        ])\n",
    "        df_pairs.name = df.name\n",
    "        print(f\"{df.name}: {len(df_pairs)} rows in pair DataFrame\")\n",
    "\n",
    "        df_pairs['FeatureMatch'] = df_pairs.apply(\n",
    "            lambda row: classify_feature_match(row['FeatureCombo1'], row['FeatureCombo2']),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # This line is now optional, since (A, A) won't be included unless you use combinations_with_replacement\n",
    "        df_pairs['FeatureMatch'] = df_pairs.apply(\n",
    "            lambda row: 'SelfPair' if row['Word1'] == row['Word2'] else row['FeatureMatch'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Call your function with a list containing one df_pairs\n",
    "        add_feature_match_specific([df_pairs], save_path_prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bf6b5933-319d-453e-8927-2bd8de1058cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing v...\n",
      "v: 780 unique unordered pairs\n",
      "v: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/v_MaxMatch.csv\n",
      "Saved: ../exp_files/v_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/v_MaxMismatch.csv\n",
      "Saved: ../exp_files/v_ValenceMatchingOnly.csv\n",
      "Processing v2...\n",
      "v2: 780 unique unordered pairs\n",
      "v2: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/v2_MaxMatch.csv\n",
      "Saved: ../exp_files/v2_ValenceMatchingOnly.csv\n",
      "Saved: ../exp_files/v2_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/v2_MaxMismatch.csv\n",
      "Processing n...\n",
      "n: 780 unique unordered pairs\n",
      "n: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/n_MaxMatch.csv\n",
      "Saved: ../exp_files/n_MaxMismatch.csv\n",
      "Saved: ../exp_files/n_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/n_ValenceMatchingOnly.csv\n",
      "Processing n2...\n",
      "n2: 780 unique unordered pairs\n",
      "n2: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/n2_MaxMatch.csv\n",
      "Saved: ../exp_files/n2_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/n2_ValenceMatchingOnly.csv\n",
      "Saved: ../exp_files/n2_MaxMismatch.csv\n",
      "Processing s...\n",
      "s: 780 unique unordered pairs\n",
      "s: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/s_MaxMatch.csv\n",
      "Saved: ../exp_files/s_ValenceMatchingOnly.csv\n",
      "Saved: ../exp_files/s_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/s_MaxMismatch.csv\n",
      "Processing a...\n",
      "a: 780 unique unordered pairs\n",
      "a: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/a_MaxMatch.csv\n",
      "Saved: ../exp_files/a_MaxMismatch.csv\n",
      "Saved: ../exp_files/a_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/a_ValenceMatchingOnly.csv\n",
      "Processing a2...\n",
      "a2: 780 unique unordered pairs\n",
      "a2: 780 rows in pair DataFrame\n",
      "Saved: ../exp_files/a2_MaxMatch.csv\n",
      "Saved: ../exp_files/a2_MaxMismatch.csv\n",
      "Saved: ../exp_files/a2_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/a2_ValenceMatchingOnly.csv\n"
     ]
    }
   ],
   "source": [
    "for df, name in zip([v, v2, n, n2, s, a, a2], ['v', 'v2', 'n', 'n2', 's', 'a', 'a2']):\n",
    "    df.name = name\n",
    "\n",
    "# Then run the function\n",
    "process_and_save_pairwise_dfs([v, v2, n, n2, s, a, a2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdadeef7-5047-4b13-9cad-e60a1a8ae13c",
   "metadata": {},
   "source": [
    "# Ordered Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc09f423-5bb8-42b6-acc2-627d3cce8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [v,v2,n,n2,s,a,a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da28007e-fa8b-4b6c-8bed-1086fdec99f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2b049a35-b17a-4e7f-8aa5-1ea0a07a8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You want to cycle only within the first n letters (A to J), \n",
    "# and then loop back around to A (not K, L, etc.) \n",
    "# but track how many loops have occurred to produce AA, BB, etc.\n",
    "\n",
    "# You want to cycle through Aâ€“J repeatedly for each Word1 group â€” just with a shifting starting letter per group.\n",
    "# The label length should stay 1 letter (Aâ€“J) \n",
    "# until the 10th unique value, \n",
    "# at which point we start doubling (AAâ€“JJ), and later tripling, and so on.\n",
    "\n",
    "def generate_group_labels_strict(n, group_index, base_letters):\n",
    "    \"\"\"\n",
    "    Generate n group labels from a base_letters subset (e.g., A-J), shifting the start\n",
    "    per group_index, and repeating letters (A, AA, AAA...) only after 10 groups.\n",
    "    \"\"\"\n",
    "    base_len = len(base_letters)\n",
    "    labels = []\n",
    "\n",
    "    for i in range(n):\n",
    "        pos = (group_index + i) % base_len\n",
    "        repeat = (group_index) // base_len + 1  # only increase length every 10 groups\n",
    "        label = base_letters[pos] * repeat\n",
    "        labels.append(label)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_feature_match_specific(df_list, save_path_prefix):\n",
    "    base_letters = list(string.ascii_uppercase[:10])  # Aâ€“J\n",
    "\n",
    "    for df in df_list:\n",
    "        base_name = getattr(df, 'name', 'df')\n",
    "        unique_values = df['FeatureMatch'].unique()\n",
    "\n",
    "        for val in unique_values:\n",
    "            split_df = df[df['FeatureMatch'] == val].copy()\n",
    "\n",
    "            group_labels = []\n",
    "            for group_index, word1_val in enumerate(split_df['Word1'].unique()):\n",
    "                group_size = (split_df['Word1'] == word1_val).sum()\n",
    "                labels = generate_group_labels_strict(group_size, group_index, base_letters)\n",
    "                group_labels.extend(labels)\n",
    "\n",
    "            split_df['Group'] = group_labels\n",
    "\n",
    "            save_path = f\"{save_path_prefix}{base_name}_{val}.csv\"\n",
    "            split_df.to_csv(save_path, index=False)\n",
    "            print(f\"Saved: {save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_and_save_pairwise_dfs(df_list, save_path_prefix=\"../exp_files/\"):\n",
    "    for df in df_list:\n",
    "        if not hasattr(df, 'name'):\n",
    "            raise ValueError(\"Each DataFrame in df_list must have a `.name` attribute.\")\n",
    "        print(f\"Processing {df.name}...\")\n",
    "\n",
    "        pairs = list(itertools.product(df.itertuples(index=False), repeat=2))\n",
    "        print(f\"{df.name}: {len(pairs)} pairs\")\n",
    "\n",
    "        df_pairs = pd.DataFrame([\n",
    "            {\n",
    "                'Word1': p1.Word,\n",
    "                'Word2': p2.Word,\n",
    "                'FeatureCombo1': p1.FeatureCombo,\n",
    "                'FeatureCombo2': p2.FeatureCombo\n",
    "            }\n",
    "            for p1, p2 in pairs\n",
    "        ])\n",
    "        df_pairs.name = df.name\n",
    "        print(f\"{df.name}: {len(df_pairs)} rows in pair DataFrame\")\n",
    "\n",
    "        df_pairs['FeatureMatch'] = df_pairs.apply(\n",
    "            lambda row: classify_feature_match(row['FeatureCombo1'], row['FeatureCombo2']),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        df_pairs['FeatureMatch'] = df_pairs.apply(\n",
    "            lambda row: 'SelfPair' if row['Word1'] == row['Word2'] else row['FeatureMatch'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Call your function with a list containing one df_pairs\n",
    "        add_feature_match_specific([df_pairs], save_path_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "58e913e3-97d8-4d82-b960-7981770ffe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing v...\n",
      "v: 1600 pairs\n",
      "v: 1600 rows in pair DataFrame\n",
      "Saved: ../exp_files/v_SelfPair.csv\n",
      "Saved: ../exp_files/v_MaxMatch.csv\n",
      "Saved: ../exp_files/v_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/v_MaxMismatch.csv\n",
      "Saved: ../exp_files/v_ValenceMatchingOnly.csv\n",
      "Processing v2...\n",
      "v2: 1600 pairs\n",
      "v2: 1600 rows in pair DataFrame\n",
      "Saved: ../exp_files/v2_SelfPair.csv\n",
      "Saved: ../exp_files/v2_MaxMatch.csv\n",
      "Saved: ../exp_files/v2_ValenceMatchingOnly.csv\n",
      "Saved: ../exp_files/v2_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/v2_MaxMismatch.csv\n",
      "Processing n...\n",
      "n: 1600 pairs\n",
      "n: 1600 rows in pair DataFrame\n",
      "Saved: ../exp_files/n_SelfPair.csv\n",
      "Saved: ../exp_files/n_MaxMatch.csv\n",
      "Saved: ../exp_files/n_MaxMismatch.csv\n",
      "Saved: ../exp_files/n_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/n_ValenceMatchingOnly.csv\n",
      "Processing n2...\n",
      "n2: 1600 pairs\n",
      "n2: 1600 rows in pair DataFrame\n",
      "Saved: ../exp_files/n2_SelfPair.csv\n",
      "Saved: ../exp_files/n2_MaxMatch.csv\n",
      "Saved: ../exp_files/n2_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/n2_ValenceMatchingOnly.csv\n",
      "Saved: ../exp_files/n2_MaxMismatch.csv\n",
      "Processing s...\n",
      "s: 1600 pairs\n",
      "s: 1600 rows in pair DataFrame\n",
      "Saved: ../exp_files/s_SelfPair.csv\n",
      "Saved: ../exp_files/s_MaxMatch.csv\n",
      "Saved: ../exp_files/s_ValenceMatchingOnly.csv\n",
      "Saved: ../exp_files/s_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/s_MaxMismatch.csv\n",
      "Processing a...\n",
      "a: 1600 pairs\n",
      "a: 1600 rows in pair DataFrame\n",
      "Saved: ../exp_files/a_SelfPair.csv\n",
      "Saved: ../exp_files/a_MaxMatch.csv\n",
      "Saved: ../exp_files/a_MaxMismatch.csv\n",
      "Saved: ../exp_files/a_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/a_ValenceMatchingOnly.csv\n",
      "Processing a2...\n",
      "a2: 1600 pairs\n",
      "a2: 1600 rows in pair DataFrame\n",
      "Saved: ../exp_files/a2_SelfPair.csv\n",
      "Saved: ../exp_files/a2_MaxMatch.csv\n",
      "Saved: ../exp_files/a2_MaxMismatch.csv\n",
      "Saved: ../exp_files/a2_ConceptualMatchingOnly.csv\n",
      "Saved: ../exp_files/a2_ValenceMatchingOnly.csv\n"
     ]
    }
   ],
   "source": [
    "for df, name in zip([v, v2, n, n2, s, a, a2], ['v', 'v2', 'n', 'n2', 's', 'a', 'a2']):\n",
    "    df.name = name\n",
    "\n",
    "# Then run the function\n",
    "process_and_save_pairwise_dfs([v, v2, n, n2, s, a, a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5d053-12bb-4d78-bea3-2f3fd1bf0525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
